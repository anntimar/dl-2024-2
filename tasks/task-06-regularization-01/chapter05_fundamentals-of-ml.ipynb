{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Fundamentals of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Generalization: The goal of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Underfitting and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Noisy training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Ambiguous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Rare features and spurious correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding white-noise channels or all-zeros channels to MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Transformação para vetorizar (28x28 -> 784)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                        # [0, 255] -> [0.0, 1.0]\n",
    "    transforms.Lambda(lambda x: x.view(-1))       # Flatten de 28x28 para 784\n",
    "])\n",
    "\n",
    "# Carregando MNIST\n",
    "mnist_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "\n",
    "# Pegando todos os dados de uma vez (já que o notebook faz tudo de forma direta)\n",
    "train_images, train_labels = next(iter(train_loader))  # shape: [60000, 784], [60000]\n",
    "train_images = train_images.numpy()                    # Convertendo para numpy (como no original)\n",
    "\n",
    "# Adicionando canal de ruído branco\n",
    "train_images_with_noise_channels = np.concatenate(\n",
    "    [train_images, np.random.random((len(train_images), 784))], axis=1)\n",
    "\n",
    "# Adicionando canal de zeros\n",
    "train_images_with_zeros_channels = np.concatenate(\n",
    "    [train_images, np.zeros((len(train_images), 784))], axis=1)\n",
    "\n",
    "# Convertendo labels para numpy também\n",
    "train_labels = train_labels.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the same model on MNIST data with noise channels or all-zero channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1568, 512)  # 784 + 784 = 1568\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # sem softmax aqui — CrossEntropyLoss cuida disso\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_model(X, y, epochs=10, batch_size=128):\n",
    "    # Split treino/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertendo para tensores do PyTorch\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    model = SimpleModel()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                preds = model(xb)\n",
    "                predicted = torch.argmax(preds, dim=1)\n",
    "                correct += (predicted == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        acc = correct / total\n",
    "        history['val_accuracy'].append(acc)\n",
    "        print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "model_noise, history_noise = train_model(train_images_with_noise_channels, train_labels)\n",
    "\n",
    "model_zeros, history_zeros = train_model(train_images_with_zeros_channels, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting a validation accuracy comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_acc_noise = history_noise[\"val_accuracy\"]\n",
    "val_acc_zeros = history_zeros[\"val_accuracy\"]\n",
    "epochs = range(1, len(val_acc_noise) + 1)\n",
    "\n",
    "plt.plot(epochs, val_acc_noise, \"b-\", label=\"Validation accuracy with noise channels\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b--\", label=\"Validation accuracy with zeros channels\")\n",
    "plt.title(\"Effect of noise channels on validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The nature of generalization in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Fitting a MNIST model with randomly shuffled labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Transformação para normalizar e achatar\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "# Carregar MNIST\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset))\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "\n",
    "# Para numpy\n",
    "train_images = train_images.numpy().astype(\"float32\")\n",
    "train_labels = train_labels.numpy()\n",
    "\n",
    "# Embaralhar os labels\n",
    "random_train_labels = train_labels.copy()\n",
    "np.random.shuffle(random_train_labels)\n",
    "\n",
    "\n",
    "# Reutilizando a função anterior\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Treinando o modelo com rótulos aleatórios\n",
    "model_random, history_random = train_model(train_images, random_train_labels, epochs=100, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### The manifold hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Interpolation as a source of generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Why deep learning works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Training data is paramount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluating machine-learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Simple hold-out validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Iterated K-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Beating a common-sense baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Things to keep in mind about model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Improving model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Tuning key gradient descent parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training a MNIST model with an incorrectly high learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset))\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "\n",
    "train_images = train_images.numpy().astype(\"float32\")\n",
    "train_labels = train_labels.numpy()\n",
    "\n",
    "def train_model(X, y, epochs=10, batch_size=128, learning_rate=0.001):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    model = SimpleModel()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                preds = model(xb)\n",
    "                predicted = torch.argmax(preds, dim=1)\n",
    "                correct += (predicted == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        acc = correct / total\n",
    "        history['val_accuracy'].append(acc)\n",
    "        print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "model_high_lr, history_high_lr = train_model(train_images, train_labels, epochs=10, batch_size=128, learning_rate=1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**The same model with a more appropriate learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instanciar o modelo\n",
    "model = SimpleModel()\n",
    "\n",
    "# Otimizador com taxa de aprendizado apropriada (1e-2)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Função de perda\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dividir treino e validação\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "# Treinamento\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validação (opcional)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            predicted = torch.argmax(preds, dim=1)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Leveraging better architecture priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Increasing model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A simple logistic regression on MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Modelo: Regressão logística\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # sem softmax (CrossEntropyLoss já aplica)\n",
    "\n",
    "# Dados já devem estar em numpy (train_images, train_labels)\n",
    "\n",
    "# Separar treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "# Instanciar modelo, otimizador, função de perda\n",
    "model = LogisticRegressionModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Histórico para plot posterior\n",
    "history_small_model = {\"val_loss\": []}\n",
    "\n",
    "# Treinamento por 20 épocas\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            val_loss_total += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss_avg = val_loss_total / len(val_loader.dataset)\n",
    "    history_small_model[\"val_loss\"].append(val_loss_avg)\n",
    "    print(f\"Epoch {epoch+1}: val_loss = {val_loss_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_loss = history_small_model[\"val_loss\"]\n",
    "epochs = range(1, len(val_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, val_loss, \"b--\", label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Definir o modelo mais \"largo\"\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 96)\n",
    "        self.fc2 = nn.Linear(96, 96)\n",
    "        self.fc3 = nn.Linear(96, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # CrossEntropyLoss cuida da softmax\n",
    "       \n",
    "# 2. Separar treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "# 3. Instanciar modelo e otimizador\n",
    "model = LargeModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Treinamento\n",
    "history_large_model = {\"val_loss\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            val_loss_total += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss_avg = val_loss_total / len(val_loader.dataset)\n",
    "    history_large_model[\"val_loss\"].append(val_loss_avg)\n",
    "    print(f\"Epoch {epoch+1}: val_loss = {val_loss_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Improving generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Dataset curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Regularizing your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Reducing the network's size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Original model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import imdb  # só para carregar o dataset original\n",
    "\n",
    "# 1. Carregar dataset IMDB\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Vetorizar os dados (one-hot manual)\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension), dtype=\"float32\")\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "train_data = vectorize_sequences(train_data, dimension=10000)\n",
    "train_labels = np.array(train_labels).astype(\"float32\")\n",
    "\n",
    "# 3. Separar treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=512)\n",
    "\n",
    "# 4. Modelo\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Saída entre 0 e 1\n",
    "        return x\n",
    "\n",
    "model = SentimentModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 5. Treinamento\n",
    "history_original = {\"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb).squeeze()\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    history_original[\"val_accuracy\"].append(acc)\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Version of the model with lower capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# 1. Separar novamente os dados (caso precise)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=512)\n",
    "\n",
    "# 2. Modelo com capacidade reduzida\n",
    "class SmallerSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 4)\n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = SmallerSentimentModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 3. Treinamento\n",
    "history_smaller_model = {\"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb).squeeze()\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    history_smaller_model[\"val_accuracy\"].append(acc)\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Version of the model with higher capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Dividir os dados em treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=512)\n",
    "\n",
    "# 2. Modelo com maior capacidade\n",
    "class LargerSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = LargerSentimentModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 3. Treinamento\n",
    "history_larger_model = {\"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb).squeeze()\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    history_larger_model[\"val_accuracy\"].append(acc)\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Adding weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding L2 weight regularization to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Divisão treino/validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=512)\n",
    "\n",
    "# 2. Modelo com arquitetura original\n",
    "class L2RegularizedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = L2RegularizedModel()\n",
    "\n",
    "# 3. Otimizador com regularização L2 (weight_decay)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), weight_decay=0.002)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 4. Treinamento\n",
    "history_l2_reg = {\"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb).squeeze()\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    history_l2_reg[\"val_accuracy\"].append(acc)\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Different weight regularizers available in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "l1_lambda = 0.001\n",
    "l1_loss = 0\n",
    "for param in model.parameters():\n",
    "    l1_loss += torch.sum(torch.abs(param))\n",
    "\n",
    "loss = loss_fn(preds, yb) + l1_lambda * l1_loss\n",
    "\n",
    "# Otimizador com L2 regularization\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), weight_decay=0.001)\n",
    "\n",
    "# L1 regularization manual dentro do treino\n",
    "l1_lambda = 0.001\n",
    "l1_loss = 0\n",
    "for param in model.parameters():\n",
    "    l1_loss += torch.sum(torch.abs(param))\n",
    "\n",
    "loss = loss_fn(preds, yb) + l1_lambda * l1_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Adding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding dropout to the IMDB model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Separar treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.4, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=512)\n",
    "\n",
    "# 2. Modelo com Dropout\n",
    "class DropoutSentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = DropoutSentimentModel()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 3. Treinamento\n",
    "history_dropout = {\"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb).squeeze()\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            correct += (predicted == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    history_dropout[\"val_accuracy\"].append(acc)\n",
    "    print(f\"Epoch {epoch+1}: val_acc = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter05_fundamentals-of-ml.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
